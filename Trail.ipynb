{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from scipy.sparse import hstack\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning text dataset\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"Data/cleanwords.csv\")\n",
    "## cleaning text in datasets\n",
    "\n",
    "print('cleaning text dataset')\n",
    "# Regex to remove all Non-Alpha Numeric and space\n",
    "splchar_removal=re.compile(r'[^?!.,:a-z\\d ]',re.IGNORECASE)\n",
    "\n",
    "# regex to replace all numerics\n",
    "replace_no=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def clean_text(text, remove_stopwords=False, stem_words=False, clean_wiki_tokens=True):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    # dirty words\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n",
    "    text = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", text)\n",
    "    \n",
    "    if clean_wiki_tokens:\n",
    "        # Drop the image\n",
    "        text = re.sub(r\"image:[a-zA-Z0-9]*\\.jpg\", \" \", text)\n",
    "        text = re.sub(r\"image:[a-zA-Z0-9]*\\.png\", \" \", text)\n",
    "        text = re.sub(r\"image:[a-zA-Z0-9]*\\.gif\", \" \", text)\n",
    "        text = re.sub(r\"image:[a-zA-Z0-9]*\\.bmp\", \" \", text)\n",
    "\n",
    "        # Drop css\n",
    "        text = re.sub(r\"#([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3})\", \" \",text)\n",
    "        text = re.sub(r\"\\{\\|[^\\}]*\\|\\}\", \" \", text)\n",
    "        \n",
    "        # Clean templates\n",
    "        text = re.sub(r\"\\[?\\[user:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[user:.*\\|\", \" \", text)        \n",
    "        text = re.sub(r\"\\[?\\[wikipedia:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[wikipedia:.*\\|\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[special:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[special:.*\\|\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[category:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[category:.*\\|\", \" \", text)\n",
    "        \n",
    "    # data frame \n",
    "    for i in range(len(df['type'])):\n",
    "        text = re.sub(df['type'][i]+ \" \" ,df['correct'][i] + \" \", text)\n",
    "\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\?\", \" ? \", text)\n",
    "    text = re.sub(r\"\\!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\\"\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = replace_no.sub(' ', text)\n",
    "    text = splchar_removal.sub('',text)\n",
    "    #splitting line to words and joining it back with single space to join words back and remove extra spaces.\n",
    "    \n",
    "    text = \" \".join(text.split())\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "\n",
    "    return (text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned.\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('Data/train.csv')\n",
    "test_df = pd.read_csv('Data/test.csv')\n",
    "list_sentences_train = train_df[\"comment_text\"].fillna(\"no comment\").values\n",
    "list_sentences_test = test_df[\"comment_text\"].fillna(\"no comment\").values\n",
    "\n",
    "comments = [clean_text(text) for text in list_sentences_train]    \n",
    "test_comments=[clean_text(text) for text in list_sentences_test]\n",
    "    \n",
    "print(\"Cleaned.\")\n",
    "\n",
    "# train_df['comment_text'] = comments\n",
    "# test_df['comment_text'] = test_comments\n",
    "# train_df.to_csv('Data/cleaned_train.csv', index=False)\n",
    "# test_df.to_csv('Data/cleaned_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=30000,\n",
       "                min_df=1, ngram_range=(1, 6), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents='unicode',\n",
       "                sublinear_tf=True, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "all_text = pd.concat([pd.Series(list_sentences_train),  pd.Series(list_sentences_test)])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=20000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(comments)\n",
    "test_word_features = word_vectorizer.transform(test_comments)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 6),\n",
    "    max_features=30000)\n",
    "char_vectorizer.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char_features = char_vectorizer.transform(comments)\n",
    "test_char_features = char_vectorizer.transform(test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])\n",
    "\n",
    "losses = []\n",
    "predictions = {'id': test_df['id']}\n",
    "for class_name in class_names:\n",
    "    train_target = train_df[class_name]\n",
    "    classifier = LogisticRegression(solver='sag')\n",
    "    classifier.fit(train_features, train_target)\n",
    "    predictions[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "    \n",
    "submission = pd.DataFrame.from_dict(predictions)\n",
    "submission.to_csv('Data/Logistic-Submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
